<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lit_saint package &mdash; SAINT Lightning 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Example Regression" href="../example_regression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> SAINT Lightning
          </a>
              <div class="version">
                0.2.3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Network Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../augmentations.html">Data Augmentation Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uncertainty.html">Uncertainty Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../example_classification.html">Example Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../example_regression.html">Example Regression</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">lit_saint package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.augmentations">lit_saint.augmentations module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.config">lit_saint.config module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.datamodule">lit_saint.datamodule module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.dataset">lit_saint.dataset module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.model">lit_saint.model module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.modules">lit_saint.modules module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.trainer">lit_saint.trainer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint.version">lit_saint.version module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-lit_saint">Module contents</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SAINT Lightning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>lit_saint package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/_generated/lit_saint.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="lit-saint-package">
<h1>lit_saint package<a class="headerlink" href="#lit-saint-package" title="Permalink to this headline"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
</section>
<section id="module-lit_saint.augmentations">
<span id="lit-saint-augmentations-module"></span><h2>lit_saint.augmentations module<a class="headerlink" href="#module-lit_saint.augmentations" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lit_saint.augmentations.cutmix">
<span class="sig-prename descclassname"><span class="pre">lit_saint.augmentations.</span></span><span class="sig-name descname"><span class="pre">cutmix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/augmentations.html#cutmix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.augmentations.cutmix" title="Permalink to this definition"></a></dt>
<dd><p>Define how apply cutmix to a tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Tensor on which apply the cutmix augmentation</p></li>
<li><p><strong>random_index</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – list of indices used to permute the tensor</p></li>
<li><p><strong>lam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – probability values have 0 in a binary random mask, so it means probability original values will</p></li>
</ul>
</dd>
</dl>
<p>be updated</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lit_saint.augmentations.get_random_index">
<span class="sig-prename descclassname"><span class="pre">lit_saint.augmentations.</span></span><span class="sig-name descname"><span class="pre">get_random_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/augmentations.html#get_random_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.augmentations.get_random_index" title="Permalink to this definition"></a></dt>
<dd><p>Given a tensor it compute random indices between 0 and the number of the first dimension</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Tensor used to get the number of rows</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lit_saint.augmentations.mixup">
<span class="sig-prename descclassname"><span class="pre">lit_saint.augmentations.</span></span><span class="sig-name descname"><span class="pre">mixup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/augmentations.html#mixup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.augmentations.mixup" title="Permalink to this definition"></a></dt>
<dd><p>It apply mixup augmentation, making a weighted average between a tensor
and some random element of the tensor taking random rows</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Tensor on which apply the mixup augmentation</p></li>
<li><p><strong>random_index</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – list of indices used to permute the tensor</p></li>
<li><p><strong>lam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – weight in the linear combination between the original values and the random permutation</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-lit_saint.config">
<span id="lit-saint-config-module"></span><h2>lit_saint.config module<a class="headerlink" href="#module-lit_saint.config" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.AttentionTypeEnum">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">AttentionTypeEnum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#AttentionTypeEnum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.AttentionTypeEnum" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.AttentionTypeEnum.col">
<span class="sig-name descname"><span class="pre">col</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'col'</span></em><a class="headerlink" href="#lit_saint.config.AttentionTypeEnum.col" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.AttentionTypeEnum.colrow">
<span class="sig-name descname"><span class="pre">colrow</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'colrow'</span></em><a class="headerlink" href="#lit_saint.config.AttentionTypeEnum.colrow" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.AttentionTypeEnum.row">
<span class="sig-name descname"><span class="pre">row</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'row'</span></em><a class="headerlink" href="#lit_saint.config.AttentionTypeEnum.row" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.AugmentationConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">AugmentationConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cutmix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">CutMixConfig(lam=0.1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixup</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">MixUpConfig(lam=0.1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#AugmentationConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.AugmentationConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the parameters used for the augmentations</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.AugmentationConfig.cutmix">
<span class="sig-name descname"><span class="pre">cutmix</span></span><em class="property"><span class="pre">:</span> <span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lit_saint.config.CutMixConfig" title="lit_saint.config.CutMixConfig"><span class="pre">lit_saint.config.CutMixConfig</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"> <span class="pre">=</span> <span class="pre">CutMixConfig(lam=0.1)</span></em><a class="headerlink" href="#lit_saint.config.AugmentationConfig.cutmix" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.AugmentationConfig.mixup">
<span class="sig-name descname"><span class="pre">mixup</span></span><em class="property"><span class="pre">:</span> <span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lit_saint.config.MixUpConfig" title="lit_saint.config.MixUpConfig"><span class="pre">lit_saint.config.MixUpConfig</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"> <span class="pre">=</span> <span class="pre">MixUpConfig(lam=0.1)</span></em><a class="headerlink" href="#lit_saint.config.AugmentationConfig.mixup" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.ConstrastiveConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">ConstrastiveConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constrastive_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">ConstrativeEnum.simsiam</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">projhead_style</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">ProjectionHeadStyleEnum.different</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nce_temp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#ConstrastiveConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.ConstrastiveConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the parameters for Contrastive pretraining task</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ConstrastiveConfig.constrastive_type">
<span class="sig-name descname"><span class="pre">constrastive_type</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.ConstrativeEnum" title="lit_saint.config.ConstrativeEnum"><span class="pre">lit_saint.config.ConstrativeEnum</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">'simsiam'</span></em><a class="headerlink" href="#lit_saint.config.ConstrastiveConfig.constrastive_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ConstrastiveConfig.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.0</span></em><a class="headerlink" href="#lit_saint.config.ConstrastiveConfig.dropout" title="Permalink to this definition"></a></dt>
<dd><p>probability dropout in projection head</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ConstrastiveConfig.nce_temp">
<span class="sig-name descname"><span class="pre">nce_temp</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.5</span></em><a class="headerlink" href="#lit_saint.config.ConstrastiveConfig.nce_temp" title="Permalink to this definition"></a></dt>
<dd><p>temperature used for the logits in case of standard constrastive type</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ConstrastiveConfig.projhead_style">
<span class="sig-name descname"><span class="pre">projhead_style</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.ProjectionHeadStyleEnum" title="lit_saint.config.ProjectionHeadStyleEnum"><span class="pre">lit_saint.config.ProjectionHeadStyleEnum</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">'different'</span></em><a class="headerlink" href="#lit_saint.config.ConstrastiveConfig.projhead_style" title="Permalink to this definition"></a></dt>
<dd><p>it is used to project embeddings</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ConstrastiveConfig.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.1</span></em><a class="headerlink" href="#lit_saint.config.ConstrastiveConfig.weight" title="Permalink to this definition"></a></dt>
<dd><p>weight of the loss for this pretraining task</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.ConstrativeEnum">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">ConstrativeEnum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#ConstrativeEnum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.ConstrativeEnum" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ConstrativeEnum.simsiam">
<span class="sig-name descname"><span class="pre">simsiam</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'simsiam'</span></em><a class="headerlink" href="#lit_saint.config.ConstrativeEnum.simsiam" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ConstrativeEnum.standard">
<span class="sig-name descname"><span class="pre">standard</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'standard'</span></em><a class="headerlink" href="#lit_saint.config.ConstrativeEnum.standard" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.CutMixConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">CutMixConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#CutMixConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.CutMixConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the parameters for CutMix augmentation</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.CutMixConfig.lam">
<span class="sig-name descname"><span class="pre">lam</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.1</span></em><a class="headerlink" href="#lit_saint.config.CutMixConfig.lam" title="Permalink to this definition"></a></dt>
<dd><p>probability original values will be updated</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.DenoisingConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">DenoisingConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight_cross_entropy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_mse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_dim_internal_sepmlp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#DenoisingConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.DenoisingConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the parameters for Denoising pretraining task</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.DenoisingConfig.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.0</span></em><a class="headerlink" href="#lit_saint.config.DenoisingConfig.dropout" title="Permalink to this definition"></a></dt>
<dd><p>probability dropout in SepMLP</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.DenoisingConfig.scale_dim_internal_sepmlp">
<span class="sig-name descname"><span class="pre">scale_dim_internal_sepmlp</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">5</span></em><a class="headerlink" href="#lit_saint.config.DenoisingConfig.scale_dim_internal_sepmlp" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.DenoisingConfig.weight_cross_entropy">
<span class="sig-name descname"><span class="pre">weight_cross_entropy</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.5</span></em><a class="headerlink" href="#lit_saint.config.DenoisingConfig.weight_cross_entropy" title="Permalink to this definition"></a></dt>
<dd><p>weight reconstruction loss for categorical features</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.DenoisingConfig.weight_mse">
<span class="sig-name descname"><span class="pre">weight_mse</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.5</span></em><a class="headerlink" href="#lit_saint.config.DenoisingConfig.weight_mse" title="Permalink to this definition"></a></dt>
<dd><p>weight reconstruction loss for continuous features</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.MixUpConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">MixUpConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#MixUpConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.MixUpConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the parameters for CutMix augmentation</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.MixUpConfig.lam">
<span class="sig-name descname"><span class="pre">lam</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.1</span></em><a class="headerlink" href="#lit_saint.config.MixUpConfig.lam" title="Permalink to this definition"></a></dt>
<dd><p>weight used for the linear combination</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.NetworkConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">NetworkConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">transformer=TransformerConfig(depth=3</span></em>, <em class="sig-param"><span class="pre">heads=1</span></em>, <em class="sig-param"><span class="pre">dropout=0.1</span></em>, <em class="sig-param"><span class="pre">attention_type=&lt;AttentionTypeEnum.col:</span> <span class="pre">'col'&gt;</span></em>, <em class="sig-param"><span class="pre">dim_head=64</span></em>, <em class="sig-param"><span class="pre">scale_dim_internal_col=4</span></em>, <em class="sig-param"><span class="pre">scale_dim_internal_row=4)</span></em>, <em class="sig-param"><span class="pre">num_workers=0</span></em>, <em class="sig-param"><span class="pre">embedding_size=10</span></em>, <em class="sig-param"><span class="pre">internal_dimension_embed_continuous=100</span></em>, <em class="sig-param"><span class="pre">dropout_embed_continuous=0.0</span></em>, <em class="sig-param"><span class="pre">batch_size=256</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#NetworkConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.NetworkConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the neural network parameters</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.NetworkConfig.batch_size">
<span class="sig-name descname"><span class="pre">batch_size</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">256</span></em><a class="headerlink" href="#lit_saint.config.NetworkConfig.batch_size" title="Permalink to this definition"></a></dt>
<dd><p>dimension of batches using by dataloaders</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.NetworkConfig.dropout_embed_continuous">
<span class="sig-name descname"><span class="pre">dropout_embed_continuous</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.0</span></em><a class="headerlink" href="#lit_saint.config.NetworkConfig.dropout_embed_continuous" title="Permalink to this definition"></a></dt>
<dd><p>dropout used to compute embedding continuous features</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.NetworkConfig.embedding_size">
<span class="sig-name descname"><span class="pre">embedding_size</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">10</span></em><a class="headerlink" href="#lit_saint.config.NetworkConfig.embedding_size" title="Permalink to this definition"></a></dt>
<dd><p>dimension of computed embeddings</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.NetworkConfig.internal_dimension_embed_continuous">
<span class="sig-name descname"><span class="pre">internal_dimension_embed_continuous</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">100</span></em><a class="headerlink" href="#lit_saint.config.NetworkConfig.internal_dimension_embed_continuous" title="Permalink to this definition"></a></dt>
<dd><p>internal dimension of the mlp used to project continuous columns</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.NetworkConfig.num_workers">
<span class="sig-name descname"><span class="pre">num_workers</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0</span></em><a class="headerlink" href="#lit_saint.config.NetworkConfig.num_workers" title="Permalink to this definition"></a></dt>
<dd><p>number of cores to use</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.NetworkConfig.transformer">
<span class="sig-name descname"><span class="pre">transformer</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.TransformerConfig" title="lit_saint.config.TransformerConfig"><span class="pre">lit_saint.config.TransformerConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">TransformerConfig(depth=3,</span> <span class="pre">heads=1,</span> <span class="pre">dropout=0.1,</span> <span class="pre">attention_type=&lt;AttentionTypeEnum.col:</span> <span class="pre">'col'&gt;,</span> <span class="pre">dim_head=64,</span> <span class="pre">scale_dim_internal_col=4,</span> <span class="pre">scale_dim_internal_row=4)</span></em><a class="headerlink" href="#lit_saint.config.NetworkConfig.transformer" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.OptimizerConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">OptimizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">learning_rate=0.0001</span></em>, <em class="sig-param"><span class="pre">other_params=&lt;factory&gt;</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#OptimizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.OptimizerConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the parameters for CutMix augmentation</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.OptimizerConfig.learning_rate">
<span class="sig-name descname"><span class="pre">learning_rate</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.0001</span></em><a class="headerlink" href="#lit_saint.config.OptimizerConfig.learning_rate" title="Permalink to this definition"></a></dt>
<dd><p>value used to specify the learning rate</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.OptimizerConfig.other_params">
<span class="sig-name descname"><span class="pre">other_params</span></span><em class="property"><span class="pre">:</span> <span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lit_saint.config.OptimizerConfig.other_params" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">PreTrainConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">aug=AugmentationConfig(cutmix=CutMixConfig(lam=0.1)</span></em>, <em class="sig-param"><span class="pre">mixup=MixUpConfig(lam=0.1))</span></em>, <em class="sig-param"><span class="pre">task=PreTrainTaskConfig(contrastive=ConstrastiveConfig(dropout=0.0</span></em>, <em class="sig-param"><span class="pre">constrastive_type=&lt;ConstrativeEnum.simsiam:</span> <span class="pre">'simsiam'&gt;</span></em>, <em class="sig-param"><span class="pre">projhead_style=&lt;ProjectionHeadStyleEnum.different:</span> <span class="pre">'different'&gt;</span></em>, <em class="sig-param"><span class="pre">nce_temp=0.5</span></em>, <em class="sig-param"><span class="pre">weight=0.1)</span></em>, <em class="sig-param"><span class="pre">denoising=DenoisingConfig(weight_cross_entropy=0.5</span></em>, <em class="sig-param"><span class="pre">weight_mse=0.5</span></em>, <em class="sig-param"><span class="pre">scale_dim_internal_sepmlp=5</span></em>, <em class="sig-param"><span class="pre">dropout=0.0))</span></em>, <em class="sig-param"><span class="pre">optimizer=OptimizerConfig(learning_rate=0.0001</span></em>, <em class="sig-param"><span class="pre">other_params={})</span></em>, <em class="sig-param"><span class="pre">epochs=2</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#PreTrainConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.PreTrainConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define parameters for the steps used during the pretraining</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainConfig.aug">
<span class="sig-name descname"><span class="pre">aug</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.AugmentationConfig" title="lit_saint.config.AugmentationConfig"><span class="pre">lit_saint.config.AugmentationConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">AugmentationConfig(cutmix=CutMixConfig(lam=0.1),</span> <span class="pre">mixup=MixUpConfig(lam=0.1))</span></em><a class="headerlink" href="#lit_saint.config.PreTrainConfig.aug" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainConfig.epochs">
<span class="sig-name descname"><span class="pre">epochs</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">2</span></em><a class="headerlink" href="#lit_saint.config.PreTrainConfig.epochs" title="Permalink to this definition"></a></dt>
<dd><p>number of epochs of training phase</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainConfig.optimizer">
<span class="sig-name descname"><span class="pre">optimizer</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.OptimizerConfig" title="lit_saint.config.OptimizerConfig"><span class="pre">lit_saint.config.OptimizerConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">OptimizerConfig(learning_rate=0.0001,</span> <span class="pre">other_params={})</span></em><a class="headerlink" href="#lit_saint.config.PreTrainConfig.optimizer" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainConfig.task">
<span class="sig-name descname"><span class="pre">task</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.PreTrainTaskConfig" title="lit_saint.config.PreTrainTaskConfig"><span class="pre">lit_saint.config.PreTrainTaskConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">PreTrainTaskConfig(contrastive=ConstrastiveConfig(dropout=0.0,</span> <span class="pre">constrastive_type=&lt;ConstrativeEnum.simsiam:</span> <span class="pre">'simsiam'&gt;,</span> <span class="pre">projhead_style=&lt;ProjectionHeadStyleEnum.different:</span> <span class="pre">'different'&gt;,</span> <span class="pre">nce_temp=0.5,</span> <span class="pre">weight=0.1),</span> <span class="pre">denoising=DenoisingConfig(weight_cross_entropy=0.5,</span> <span class="pre">weight_mse=0.5,</span> <span class="pre">scale_dim_internal_sepmlp=5,</span> <span class="pre">dropout=0.0))</span></em><a class="headerlink" href="#lit_saint.config.PreTrainConfig.task" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainTaskConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">PreTrainTaskConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">contrastive=ConstrastiveConfig(dropout=0.0</span></em>, <em class="sig-param"><span class="pre">constrastive_type=&lt;ConstrativeEnum.simsiam:</span> <span class="pre">'simsiam'&gt;</span></em>, <em class="sig-param"><span class="pre">projhead_style=&lt;ProjectionHeadStyleEnum.different:</span> <span class="pre">'different'&gt;</span></em>, <em class="sig-param"><span class="pre">nce_temp=0.5</span></em>, <em class="sig-param"><span class="pre">weight=0.1)</span></em>, <em class="sig-param"><span class="pre">denoising=DenoisingConfig(weight_cross_entropy=0.5</span></em>, <em class="sig-param"><span class="pre">weight_mse=0.5</span></em>, <em class="sig-param"><span class="pre">scale_dim_internal_sepmlp=5</span></em>, <em class="sig-param"><span class="pre">dropout=0.0)</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#PreTrainTaskConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.PreTrainTaskConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define the parameters used for pretraining tasks</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainTaskConfig.contrastive">
<span class="sig-name descname"><span class="pre">contrastive</span></span><em class="property"><span class="pre">:</span> <span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lit_saint.config.ConstrastiveConfig" title="lit_saint.config.ConstrastiveConfig"><span class="pre">lit_saint.config.ConstrastiveConfig</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"> <span class="pre">=</span> <span class="pre">ConstrastiveConfig(dropout=0.0,</span> <span class="pre">constrastive_type=&lt;ConstrativeEnum.simsiam:</span> <span class="pre">'simsiam'&gt;,</span> <span class="pre">projhead_style=&lt;ProjectionHeadStyleEnum.different:</span> <span class="pre">'different'&gt;,</span> <span class="pre">nce_temp=0.5,</span> <span class="pre">weight=0.1)</span></em><a class="headerlink" href="#lit_saint.config.PreTrainTaskConfig.contrastive" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.PreTrainTaskConfig.denoising">
<span class="sig-name descname"><span class="pre">denoising</span></span><em class="property"><span class="pre">:</span> <span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lit_saint.config.DenoisingConfig" title="lit_saint.config.DenoisingConfig"><span class="pre">lit_saint.config.DenoisingConfig</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"> <span class="pre">=</span> <span class="pre">DenoisingConfig(weight_cross_entropy=0.5,</span> <span class="pre">weight_mse=0.5,</span> <span class="pre">scale_dim_internal_sepmlp=5,</span> <span class="pre">dropout=0.0)</span></em><a class="headerlink" href="#lit_saint.config.PreTrainTaskConfig.denoising" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.ProjectionHeadStyleEnum">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">ProjectionHeadStyleEnum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#ProjectionHeadStyleEnum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.ProjectionHeadStyleEnum" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ProjectionHeadStyleEnum.different">
<span class="sig-name descname"><span class="pre">different</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'different'</span></em><a class="headerlink" href="#lit_saint.config.ProjectionHeadStyleEnum.different" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.ProjectionHeadStyleEnum.same">
<span class="sig-name descname"><span class="pre">same</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'same'</span></em><a class="headerlink" href="#lit_saint.config.ProjectionHeadStyleEnum.same" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.SaintConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">SaintConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">network=NetworkConfig(transformer=TransformerConfig(depth=3</span></em>, <em class="sig-param"><span class="pre">heads=1</span></em>, <em class="sig-param"><span class="pre">dropout=0.1</span></em>, <em class="sig-param"><span class="pre">attention_type=&lt;AttentionTypeEnum.col:</span> <span class="pre">'col'&gt;</span></em>, <em class="sig-param"><span class="pre">dim_head=64</span></em>, <em class="sig-param"><span class="pre">scale_dim_internal_col=4</span></em>, <em class="sig-param"><span class="pre">scale_dim_internal_row=4)</span></em>, <em class="sig-param"><span class="pre">num_workers=0</span></em>, <em class="sig-param"><span class="pre">embedding_size=10</span></em>, <em class="sig-param"><span class="pre">internal_dimension_embed_continuous=100</span></em>, <em class="sig-param"><span class="pre">dropout_embed_continuous=0.0</span></em>, <em class="sig-param"><span class="pre">batch_size=256)</span></em>, <em class="sig-param"><span class="pre">pretrain=PreTrainConfig(aug=AugmentationConfig(cutmix=CutMixConfig(lam=0.1)</span></em>, <em class="sig-param"><span class="pre">mixup=MixUpConfig(lam=0.1))</span></em>, <em class="sig-param"><span class="pre">task=PreTrainTaskConfig(contrastive=ConstrastiveConfig(dropout=0.0</span></em>, <em class="sig-param"><span class="pre">constrastive_type=&lt;ConstrativeEnum.simsiam:</span> <span class="pre">'simsiam'&gt;</span></em>, <em class="sig-param"><span class="pre">projhead_style=&lt;ProjectionHeadStyleEnum.different:</span> <span class="pre">'different'&gt;</span></em>, <em class="sig-param"><span class="pre">nce_temp=0.5</span></em>, <em class="sig-param"><span class="pre">weight=0.1)</span></em>, <em class="sig-param"><span class="pre">denoising=DenoisingConfig(weight_cross_entropy=0.5</span></em>, <em class="sig-param"><span class="pre">weight_mse=0.5</span></em>, <em class="sig-param"><span class="pre">scale_dim_internal_sepmlp=5</span></em>, <em class="sig-param"><span class="pre">dropout=0.0))</span></em>, <em class="sig-param"><span class="pre">optimizer=OptimizerConfig(learning_rate=0.0001</span></em>, <em class="sig-param"><span class="pre">other_params={})</span></em>, <em class="sig-param"><span class="pre">epochs=2)</span></em>, <em class="sig-param"><span class="pre">train=TrainConfig(internal_dimension_output_layer=20</span></em>, <em class="sig-param"><span class="pre">mlpfory_dropout=0.0</span></em>, <em class="sig-param"><span class="pre">epochs=10</span></em>, <em class="sig-param"><span class="pre">optimizer=OptimizerConfig(learning_rate=0.0001</span></em>, <em class="sig-param"><span class="pre">other_params={}))</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#SaintConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.SaintConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define all the parameters used in SAINT</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.SaintConfig.network">
<span class="sig-name descname"><span class="pre">network</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.NetworkConfig" title="lit_saint.config.NetworkConfig"><span class="pre">lit_saint.config.NetworkConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">NetworkConfig(transformer=TransformerConfig(depth=3,</span> <span class="pre">heads=1,</span> <span class="pre">dropout=0.1,</span> <span class="pre">attention_type=&lt;AttentionTypeEnum.col:</span> <span class="pre">'col'&gt;,</span> <span class="pre">dim_head=64,</span> <span class="pre">scale_dim_internal_col=4,</span> <span class="pre">scale_dim_internal_row=4),</span> <span class="pre">num_workers=0,</span> <span class="pre">embedding_size=10,</span> <span class="pre">internal_dimension_embed_continuous=100,</span> <span class="pre">dropout_embed_continuous=0.0,</span> <span class="pre">batch_size=256)</span></em><a class="headerlink" href="#lit_saint.config.SaintConfig.network" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.SaintConfig.pretrain">
<span class="sig-name descname"><span class="pre">pretrain</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.PreTrainConfig" title="lit_saint.config.PreTrainConfig"><span class="pre">lit_saint.config.PreTrainConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">PreTrainConfig(aug=AugmentationConfig(cutmix=CutMixConfig(lam=0.1),</span> <span class="pre">mixup=MixUpConfig(lam=0.1)),</span> <span class="pre">task=PreTrainTaskConfig(contrastive=ConstrastiveConfig(dropout=0.0,</span> <span class="pre">constrastive_type=&lt;ConstrativeEnum.simsiam:</span> <span class="pre">'simsiam'&gt;,</span> <span class="pre">projhead_style=&lt;ProjectionHeadStyleEnum.different:</span> <span class="pre">'different'&gt;,</span> <span class="pre">nce_temp=0.5,</span> <span class="pre">weight=0.1),</span> <span class="pre">denoising=DenoisingConfig(weight_cross_entropy=0.5,</span> <span class="pre">weight_mse=0.5,</span> <span class="pre">scale_dim_internal_sepmlp=5,</span> <span class="pre">dropout=0.0)),</span> <span class="pre">optimizer=OptimizerConfig(learning_rate=0.0001,</span> <span class="pre">other_params={}),</span> <span class="pre">epochs=2)</span></em><a class="headerlink" href="#lit_saint.config.SaintConfig.pretrain" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.SaintConfig.train">
<span class="sig-name descname"><span class="pre">train</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.TrainConfig" title="lit_saint.config.TrainConfig"><span class="pre">lit_saint.config.TrainConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">TrainConfig(internal_dimension_output_layer=20,</span> <span class="pre">mlpfory_dropout=0.0,</span> <span class="pre">epochs=10,</span> <span class="pre">optimizer=OptimizerConfig(learning_rate=0.0001,</span> <span class="pre">other_params={}))</span></em><a class="headerlink" href="#lit_saint.config.SaintConfig.train" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.TrainConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">TrainConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">internal_dimension_output_layer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlpfory_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">OptimizerConfig(learning_rate=0.0001,</span> <span class="pre">other_params={})</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#TrainConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.TrainConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Define parameters for the steps used during the training</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TrainConfig.epochs">
<span class="sig-name descname"><span class="pre">epochs</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">10</span></em><a class="headerlink" href="#lit_saint.config.TrainConfig.epochs" title="Permalink to this definition"></a></dt>
<dd><p>number of epochs of training phase</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TrainConfig.internal_dimension_output_layer">
<span class="sig-name descname"><span class="pre">internal_dimension_output_layer</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">20</span></em><a class="headerlink" href="#lit_saint.config.TrainConfig.internal_dimension_output_layer" title="Permalink to this definition"></a></dt>
<dd><p>internal dimension of the MLP that compute the output</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TrainConfig.mlpfory_dropout">
<span class="sig-name descname"><span class="pre">mlpfory_dropout</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.0</span></em><a class="headerlink" href="#lit_saint.config.TrainConfig.mlpfory_dropout" title="Permalink to this definition"></a></dt>
<dd><p>probability dropout in the the MLP used for prediction</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TrainConfig.optimizer">
<span class="sig-name descname"><span class="pre">optimizer</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.OptimizerConfig" title="lit_saint.config.OptimizerConfig"><span class="pre">lit_saint.config.OptimizerConfig</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">OptimizerConfig(learning_rate=0.0001,</span> <span class="pre">other_params={})</span></em><a class="headerlink" href="#lit_saint.config.TrainConfig.optimizer" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.config.</span></span><span class="sig-name descname"><span class="pre">TransformerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AttentionTypeEnum.col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_dim_internal_col</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_dim_internal_row</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/config.html#TransformerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.config.TransformerConfig" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig.attention_type">
<span class="sig-name descname"><span class="pre">attention_type</span></span><em class="property"><span class="pre">:</span> <a class="reference internal" href="#lit_saint.config.AttentionTypeEnum" title="lit_saint.config.AttentionTypeEnum"><span class="pre">lit_saint.config.AttentionTypeEnum</span></a></em><em class="property"> <span class="pre">=</span> <span class="pre">'col'</span></em><a class="headerlink" href="#lit_saint.config.TransformerConfig.attention_type" title="Permalink to this definition"></a></dt>
<dd><p>type of attention</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig.depth">
<span class="sig-name descname"><span class="pre">depth</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">3</span></em><a class="headerlink" href="#lit_saint.config.TransformerConfig.depth" title="Permalink to this definition"></a></dt>
<dd><p>number of attention blocks used in the transformer</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig.dim_head">
<span class="sig-name descname"><span class="pre">dim_head</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">64</span></em><a class="headerlink" href="#lit_saint.config.TransformerConfig.dim_head" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">0.1</span></em><a class="headerlink" href="#lit_saint.config.TransformerConfig.dropout" title="Permalink to this definition"></a></dt>
<dd><p>probability dropout in the transformer</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig.heads">
<span class="sig-name descname"><span class="pre">heads</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><em class="property"> <span class="pre">=</span> <span class="pre">1</span></em><a class="headerlink" href="#lit_saint.config.TransformerConfig.heads" title="Permalink to this definition"></a></dt>
<dd><p>number of heads used in the transformer</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig.scale_dim_internal_col">
<span class="sig-name descname"><span class="pre">scale_dim_internal_col</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">4</span></em><a class="headerlink" href="#lit_saint.config.TransformerConfig.scale_dim_internal_col" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.config.TransformerConfig.scale_dim_internal_row">
<span class="sig-name descname"><span class="pre">scale_dim_internal_row</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><em class="property"> <span class="pre">=</span> <span class="pre">4</span></em><a class="headerlink" href="#lit_saint.config.TransformerConfig.scale_dim_internal_row" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-lit_saint.datamodule">
<span id="lit-saint-datamodule-module"></span><h2>lit_saint.datamodule module<a class="headerlink" href="#module-lit_saint.datamodule" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.datamodule.</span></span><span class="sig-name descname"><span class="pre">SaintDatamodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_column</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_loader_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.datamodule.LightningDataModule</span></code></p>
<p>It preprocess the data, doing LabelEncoding for the categorical values and fitting a StandardScaler
for the numerical columns on the training set. And it splits the data and defines the dataloaders</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – contains the data that will be used by the dataLoaders</p></li>
<li><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – name of the target column</p></li>
<li><p><strong>split_column</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – name of the column used to split the data</p></li>
<li><p><strong>data_loader_params</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – parameters used to configure the DataLoader</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.NAN_LABEL">
<span class="sig-name descname"><span class="pre">NAN_LABEL</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'SAINT_NAN'</span></em><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.NAN_LABEL" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.predict_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.predict_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Function that loads the dataset for the prediction.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.prep">
<span class="sig-name descname"><span class="pre">prep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.prep"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.prep" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>It find the indexes for each categorical and continuous columns, and for each categorical it</dt><dd><p>applies Label Encoding in order to convert them in integers and save the number of classes for each
categorical column</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>df</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – contains the data that need to be processed</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.prep_categorical_columns">
<span class="sig-name descname"><span class="pre">prep_categorical_columns</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.prep_categorical_columns"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.prep_categorical_columns" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.prep_continuous_columns">
<span class="sig-name descname"><span class="pre">prep_continuous_columns</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.prep_continuous_columns"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.prep_continuous_columns" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.scaler_continuous_columns">
<span class="sig-name descname"><span class="pre">scaler_continuous_columns</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.scaler_continuous_columns"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.scaler_continuous_columns" title="Permalink to this definition"></a></dt>
<dd><p>Fit a StandardScaler for each continuos columns on the training set</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – contains the data that need to be processed</p></li>
<li><p><strong>split_column</strong> – name of column used to split the data</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.set_predict_set">
<span class="sig-name descname"><span class="pre">set_predict_set</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.set_predict_set"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.set_predict_set" title="Permalink to this definition"></a></dt>
<dd><p>Tranform the categorical columns using the OrdinalEncoders fitted before the training and
save the dataframe in order to make the predictions</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>df</strong> – The data that will be used to make some predictions</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.set_pretraining">
<span class="sig-name descname"><span class="pre">set_pretraining</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretraining</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.set_pretraining"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.set_pretraining" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.test_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.test_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Function that loads the validation set.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.train_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.train_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Function that loads the train set.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.datamodule.SaintDatamodule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/datamodule.html#SaintDatamodule.val_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.datamodule.SaintDatamodule.val_dataloader" title="Permalink to this definition"></a></dt>
<dd><p>Function that loads the validation set.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-lit_saint.dataset">
<span id="lit-saint-dataset-module"></span><h2>lit_saint.dataset module<a class="headerlink" href="#module-lit_saint.dataset" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.dataset.SaintDataset">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.dataset.</span></span><span class="sig-name descname"><span class="pre">SaintDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_cols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">con_cols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_categorical</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/dataset.html#SaintDataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.dataset.SaintDataset" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.T_co</span></code>]</p>
<p>Contains the data that will be processed in batches, and divide them in categorical and continuous</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – Dataframe containing the data to use for the batches</p></li>
<li><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the target column</p></li>
<li><p><strong>cat_cols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – List of indices of categorical columns</p></li>
<li><p><strong>target_categorical</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True the target is categorical, so it is a classification problem</p></li>
<li><p><strong>con_cols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – List of indices of continuous columns</p></li>
<li><p><strong>scaler</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerMixin</span></code>) – a scikit learn scaler used to transform the continuous columns</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-lit_saint.model">
<span id="lit-saint-model-module"></span><h2>lit_saint.model module<a class="headerlink" href="#module-lit_saint.model" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.model.Saint">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.model.</span></span><span class="sig-name descname"><span class="pre">Saint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">categories</span></em>, <em class="sig-param"><span class="pre">continuous</span></em>, <em class="sig-param"><span class="pre">dim_target</span></em>, <em class="sig-param"><span class="pre">config</span></em>, <em class="sig-param"><span class="pre">metrics=None</span></em>, <em class="sig-param"><span class="pre">metrics_single_class=True</span></em>, <em class="sig-param"><span class="pre">optimizer=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></em>, <em class="sig-param"><span class="pre">loss_fn=None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<p>Contains the definition of the network and how to execute the pretraining tasks, and how perform
the training, validation and test steps</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>categories</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – List with the number of unique values for each categorical column</p></li>
<li><p><strong>continuous</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – List of indices with continuous columns</p></li>
<li><p><strong>dim_target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – if categorical represent number of classes of the target otherwise is 1</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="#lit_saint.config.SaintConfig" title="lit_saint.config.SaintConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SaintConfig</span></code></a>) – configuration of the model</p></li>
<li><p><strong>metrics</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>]]) – Dictionary containing custom metrics to compute during training loop</p></li>
<li><p><strong>metrics_single_class</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – boolean flag, if True the metrics are computed separately for each class</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – custom optimizer to compute gradient of the network</p></li>
<li><p><strong>loss_fn</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – custom loss function to be optimized</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.compute_metrics">
<span class="sig-name descname"><span class="pre">compute_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metrics</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.compute_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.compute_metrics" title="Permalink to this definition"></a></dt>
<dd><p>Compute custom metrics during training loop</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metrics</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>]) – Metrics to compute</p></li>
<li><p><strong>y_pred</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – predicted value</p></li>
<li><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – true value</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.configure_optimizers" title="Permalink to this definition"></a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="simple">
<dt>Return:</dt><dd><p>Any of these 6 options.</p>
<ul class="simple">
<li><p><strong>Single optimizer</strong>.</p></li>
<li><p><strong>List or Tuple</strong> of optimizers.</p></li>
<li><p><strong>Two lists</strong> - The first list has multiple optimizers, and the second has multiple LR schedulers
(or multiple <code class="docutils literal notranslate"><span class="pre">lr_dict</span></code>).</p></li>
<li><p><strong>Dictionary</strong>, with an <code class="docutils literal notranslate"><span class="pre">&quot;optimizer&quot;</span></code> key, and (optionally) a <code class="docutils literal notranslate"><span class="pre">&quot;lr_scheduler&quot;</span></code>
key whose value is a single LR scheduler or <code class="docutils literal notranslate"><span class="pre">lr_dict</span></code>.</p></li>
<li><p><strong>Tuple of dictionaries</strong> as described above, with an optional <code class="docutils literal notranslate"><span class="pre">&quot;frequency&quot;</span></code> key.</p></li>
<li><p><strong>None</strong> - Fit will run without any optimizer.</p></li>
</ul>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">lr_dict</span></code> is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># REQUIRED: The scheduler instance</span>
    <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="c1"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
    <span class="c1"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
    <span class="c1"># updates it after a optimizer update.</span>
    <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="c1"># How many epochs/steps should pass between calls to</span>
    <span class="c1"># `scheduler.step()`. 1 corresponds to updating the learning</span>
    <span class="c1"># rate after every epoch/step.</span>
    <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="c1"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
    <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
    <span class="c1"># If set to `True`, will enforce that the value specified &#39;monitor&#39;</span>
    <span class="c1"># is available when the scheduler is updated, thus stopping</span>
    <span class="c1"># training if not found. If set to `False`, it will only produce a warning</span>
    <span class="s2">&quot;strict&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># If using the `LearningRateMonitor` callback to monitor the</span>
    <span class="c1"># learning rate progress, this keyword can be used to specify</span>
    <span class="c1"># a custom logged name</span>
    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When there are schedulers in which the <code class="docutils literal notranslate"><span class="pre">.step()</span></code> method is conditioned on a value, such as the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.ReduceLROnPlateau</span></code> scheduler, Lightning requires that the <code class="docutils literal notranslate"><span class="pre">lr_dict</span></code>
contains the keyword <code class="docutils literal notranslate"><span class="pre">&quot;monitor&quot;</span></code> set to the metric name that the scheduler should be conditioned on.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The ReduceLROnPlateau scheduler requires a monitor</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">,</span>
        <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
            <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;metric_to_track&quot;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">}</span>


<span class="c1"># In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer1</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">optimizer2</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler1</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer1</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler2</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer2</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer1</span><span class="p">,</span>
            <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">scheduler1</span><span class="p">,</span>
                <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;metric_to_track&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer2</span><span class="p">,</span> <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="n">scheduler2</span><span class="p">},</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Metrics can be made available to monitor by simply logging it using
<code class="docutils literal notranslate"><span class="pre">self.log('metric_to_track',</span> <span class="pre">metric_val)</span></code> in your <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>.</p>
<dl>
<dt>Note:</dt><dd><p>The <code class="docutils literal notranslate"><span class="pre">frequency</span></code> value specified in a dict along with the <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> key is an int corresponding
to the number of sequential batches optimized with the specific optimizer.
It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:</p>
<blockquote>
<div><ul class="simple">
<li><p>In the former case, all optimizers will operate on the given batch in each optimization step.</p></li>
<li><p>In the latter, only one optimizer will operate on the given batch at every step.</p></li>
</ul>
</div></blockquote>
<p>This is different from the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> value specified in the <code class="docutils literal notranslate"><span class="pre">lr_dict</span></code> mentioned above.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer_one</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">optimizer_two</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer_one</span><span class="p">,</span> <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer_two</span><span class="p">,</span> <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span>
    <span class="p">]</span>
</pre></div>
</div>
<p>In this example, the first optimizer will be used for the first 5 steps,
the second optimizer for the next 10 steps and that cycle will continue.
If an LR scheduler is specified for an optimizer using the <code class="docutils literal notranslate"><span class="pre">lr_scheduler</span></code> key in the above dict,
the scheduler will only be updated when its optimizer is being used.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases. no learning rate scheduler</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">dis_sch</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">dis_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">dis_sch</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="c1"># each optimizer has its own scheduler</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sch</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
        <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span>  <span class="c1"># called after each training step</span>
    <span class="p">}</span>
    <span class="n">dis_sch</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">dis_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sch</span><span class="p">,</span> <span class="n">dis_sch</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note:</dt><dd><p>Some things to know:</p>
<ul class="simple">
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically handle the optimizers.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#lit_saint.model.Saint.training_step" title="lit_saint.model.Saint.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional <code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code>, Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer
at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule,
override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_categ</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_cont</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.forward" title="Permalink to this definition"></a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p><a href="#id1"><span class="problematic" id="id2">*</span></a>args: Whatever you decide to pass into the forward method.
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Keyword arguments are also possible.</p>
</dd>
<dt>Return:</dt><dd><p>Your model’s output</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.predict_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.predict_step" title="Permalink to this definition"></a></dt>
<dd><p>Step function called during <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>.
By default, it calls <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>.
Override to add any processing logic.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code> should be used while using a spawn
based accelerator. This happens for <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator=&quot;ddp_spawn&quot;)</span></code>
or training on 8 TPU cores with <code class="docutils literal notranslate"><span class="pre">Trainer(tpu_cores=8)</span></code> as predictions won’t be returned.</p>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">predicts_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">dm</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dm</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Args:</dt><dd><p>batch: Current batch
batch_idx: Index of current batch
dataloader_idx: Index of the current dataloader</p>
</dd>
<dt>Return:</dt><dd><p>Predicted output</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.pretraining_step">
<span class="sig-name descname"><span class="pre">pretraining_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_categ</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_cont</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.pretraining_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.pretraining_step" title="Permalink to this definition"></a></dt>
<dd><p>Defines all the step that must be sued in the pretraing step and return the computed loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_categ</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – values of categorical features</p></li>
<li><p><strong>x_cont</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – values of continuous features</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.set_mcdropout">
<span class="sig-name descname"><span class="pre">set_mcdropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mc_dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.set_mcdropout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.set_mcdropout" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.set_pretraining">
<span class="sig-name descname"><span class="pre">set_pretraining</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretraining</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.set_pretraining"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.set_pretraining" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.shared_step">
<span class="sig-name descname"><span class="pre">shared_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.shared_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.shared_step" title="Permalink to this definition"></a></dt>
<dd><p>It define the commons step executed during training, validation and test</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – Contains a batch of data</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.test_step" title="Permalink to this definition"></a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>batch (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]):</dt><dd><p>The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p>
</dd>
</dl>
<p>batch_idx (int): The index of this batch.
dataloader_idx (int): The index of the dataloader that produced this batch</p>
<blockquote>
<div><p>(only if multiple test dataloaders used).</p>
</div></blockquote>
</dd>
<dt>Return:</dt><dd><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>


<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#lit_saint.model.Saint.test_step" title="lit_saint.model.Saint.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note:</dt><dd><p>If you don’t need to test you don’t need to implement this method.</p>
</dd>
<dt>Note:</dt><dd><p>When the <a class="reference internal" href="#lit_saint.model.Saint.test_step" title="lit_saint.model.Saint.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.model.Saint.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.model.Saint.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.training_step" title="Permalink to this definition"></a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>batch (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]):</dt><dd><p>The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p>
</dd>
</dl>
<p>batch_idx (int): Integer displaying index of this batch
optimizer_idx (int): When using multiple optimizers, this argument will also be present.
hiddens(<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>): Passed in if</p>
<blockquote>
<div><p><a href="#id5"><span class="problematic" id="id6">:paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps`</span></a> &gt; 0.</p>
</div></blockquote>
</dd>
<dt>Return:</dt><dd><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch. This is only for automatic optimization.</dt><dd><p>This is not supported for multi-GPU or TPU, or using <code class="docutils literal notranslate"><span class="pre">DeepSpeed</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
        <span class="o">...</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;hiddens&quot;</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note:</dt><dd><p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.model.Saint.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/model.html#Saint.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.model.Saint.validation_step" title="Permalink to this definition"></a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>batch (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]):</dt><dd><p>The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p>
</dd>
</dl>
<p>batch_idx (int): The index of this batch
dataloader_idx (int): The index of the dataloader that produced this batch</p>
<blockquote>
<div><p>(only if multiple val dataloaders used)</p>
</div></blockquote>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Validation will skip to the next batch</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s2">&quot;validation_step_end&quot;</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>


<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#lit_saint.model.Saint.validation_step" title="lit_saint.model.Saint.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note:</dt><dd><p>If you don’t need to validate you don’t need to implement this method.</p>
</dd>
<dt>Note:</dt><dd><p>When the <a class="reference internal" href="#lit_saint.model.Saint.validation_step" title="lit_saint.model.Saint.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-lit_saint.modules">
<span id="lit-saint-modules-module"></span><h2>lit_saint.modules module<a class="headerlink" href="#module-lit_saint.modules" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.modules.Attention">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.modules.</span></span><span class="sig-name descname"><span class="pre">Attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_head</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#Attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.Attention" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Module that implements self attention</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – dimension embedding used in input</p></li>
<li><p><strong>heads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – number of heads for the Multi Head Attention</p></li>
<li><p><strong>dim_head</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – output dimension given an head</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.Attention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#Attention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.Attention.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.modules.Attention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.modules.Attention.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.modules.GEGLU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.modules.</span></span><span class="sig-name descname"><span class="pre">GEGLU</span></span><a class="reference internal" href="../_modules/lit_saint/modules.html#GEGLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.GEGLU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Gated GELU, it splits a tensor in two slices based on the last dimension, and then multiply the
first half and the gelu of the second half</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.GEGLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#GEGLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.GEGLU.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.modules.GEGLU.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.modules.GEGLU.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.modules.PreNorm">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.modules.</span></span><span class="sig-name descname"><span class="pre">PreNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#PreNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.PreNorm" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Apply Layer Normalization before a Module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> – dimension of embedding in input to the module</p></li>
<li><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>) – pytorch Module that will be applied after the normalization layer</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.PreNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#PreNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.PreNorm.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.modules.PreNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.modules.PreNorm.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.modules.Residual">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.modules.</span></span><span class="sig-name descname"><span class="pre">Residual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#Residual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.Residual" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Define a residual block given a Pytorch Module, used for the skip connection</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>) – pytorch Module that will be used in order to create the skip connection</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.Residual.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#Residual.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.Residual.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.modules.Residual.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.modules.Residual.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.modules.RowColTransformer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.modules.</span></span><span class="sig-name descname"><span class="pre">RowColTransformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nfeats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_head</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_dim_internal_col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_dim_internal_row</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">style</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#RowColTransformer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.RowColTransformer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Module use to define RowColTransformer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – dimension of embedding in input to the module</p></li>
<li><p><strong>nfeats</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – total number of features, needed for the intersample attention</p></li>
<li><p><strong>depth</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – depth of the network, this imply how many times is applied the attention</p></li>
<li><p><strong>heads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – number of heads for the Multi Head Attention</p></li>
<li><p><strong>dim_head</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – dimension of embedding used in each head</p></li>
<li><p><strong>ff_dropout</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – probability used in the dropout layer of the feed forward layers applied after the attention</p></li>
<li><p><strong>scale_dim_internal_col</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – scale factor input dimension in order to obtain the output dimension of the</p></li>
</ul>
</dd>
</dl>
<p>first linear layer in case of style col
:type scale_dim_internal_row: <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>
:param scale_dim_internal_row: scale factor input dimension in order to obtain the output dimension of the
first linear layer in case of style row
:type style: <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>
:param style: can be col, row, or colrow</p>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.RowColTransformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_cont</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#RowColTransformer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.RowColTransformer.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.RowColTransformer.forward_col">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">forward_col</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#RowColTransformer.forward_col"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.RowColTransformer.forward_col" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.RowColTransformer.forward_row">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">forward_row</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#RowColTransformer.forward_row"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.RowColTransformer.forward_row" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.modules.RowColTransformer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.modules.RowColTransformer.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.modules.SepMLP">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.modules.</span></span><span class="sig-name descname"><span class="pre">SepMLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_out_for_each_feat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_dim_internal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#SepMLP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.SepMLP" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Module that implements a separable MLP, this means that for each feature is used a different SimpleMLP</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – dimension of embedding in input to the module</p></li>
<li><p><strong>dim_out_for_each_feat</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – output dimension for each SimpleMLP</p></li>
<li><p><strong>scale_dim_internal</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – scale factor input dimension in order to obtain the output dimension of the</p></li>
</ul>
</dd>
</dl>
<p>first linear layer
:type dropout: <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>
:param dropout: probability used in the dropout layer</p>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.SepMLP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#SepMLP.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.SepMLP.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.modules.SepMLP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.modules.SepMLP.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.modules.SimpleMLP">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.modules.</span></span><span class="sig-name descname"><span class="pre">SimpleMLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_internal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_module</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#SimpleMLP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.SimpleMLP" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Module that implements a MLP that consists of 2 linear layers with an activation layer
and dropout in between them. In case of GEGLU activation function, there is a multiplier that
allow to divide the tensor in 2 parts</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – dimension of embedding in input to the module</p></li>
<li><p><strong>dim_internal</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – dimension after the first linear layer</p></li>
<li><p><strong>dim_out</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – output dimension</p></li>
<li><p><strong>activation_module</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>) – module with the activation function to use between the 2 linear layers</p></li>
<li><p><strong>dropout</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – probability used in the dropout layer</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.modules.SimpleMLP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/modules.html#SimpleMLP.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.modules.SimpleMLP.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lit_saint.modules.SimpleMLP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#lit_saint.modules.SimpleMLP.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-lit_saint.trainer">
<span id="lit-saint-trainer-module"></span><h2>lit_saint.trainer module<a class="headerlink" href="#module-lit_saint.trainer" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lit_saint.trainer.SaintTrainer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">lit_saint.trainer.</span></span><span class="sig-name descname"><span class="pre">SaintTrainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrainer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/trainer.html#SaintTrainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.trainer.SaintTrainer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class simplify how to train and make predictions using Saint</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrainer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code>]) – The Trainer that will be used during the pretraining phase</p></li>
<li><p><strong>trainer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code>]) – The Trainer that will be used during the training and preciction phase</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.trainer.SaintTrainer.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_pretraining</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/trainer.html#SaintTrainer.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.trainer.SaintTrainer.fit" title="Permalink to this definition"></a></dt>
<dd><p>Runs the full optimization routine.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#lit_saint.model.Saint" title="lit_saint.model.Saint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Saint</span></code></a>) – instance of Saint that will be used for the training</p></li>
<li><p><strong>datamodule</strong> (<a class="reference internal" href="#lit_saint.datamodule.SaintDatamodule" title="lit_saint.datamodule.SaintDatamodule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SaintDatamodule</span></code></a>) – instance of SaintDataModule that contains the data for the training</p></li>
<li><p><strong>enable_pretraining</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – boolean flag, if True the pretraining will be executed otherwise only the training</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.trainer.SaintTrainer.get_model_from_checkpoint">
<span class="sig-name descname"><span class="pre">get_model_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pretraining</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/trainer.html#SaintTrainer.get_model_from_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.trainer.SaintTrainer.get_model_from_checkpoint" title="Permalink to this definition"></a></dt>
<dd><p>Function that load the best checkpoint and update the model inplace</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#lit_saint.model.Saint" title="lit_saint.model.Saint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Saint</span></code></a>) – instance of Saint that will be updated by the checkpoint</p></li>
<li><p><strong>pretraining</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – boolean flag if true the checkpoint will be taken from the pretrainer</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.trainer.SaintTrainer.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mc_dropout_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/trainer.html#SaintTrainer.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.trainer.SaintTrainer.predict" title="Permalink to this definition"></a></dt>
<dd><p>Separates from fit to make sure you never run on your predictions set until you want to.
This will call the model forward function to compute predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#lit_saint.model.Saint" title="lit_saint.model.Saint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Saint</span></code></a>) – model used for the prediction</p></li>
<li><p><strong>datamodule</strong> (<a class="reference internal" href="#lit_saint.datamodule.SaintDatamodule" title="lit_saint.datamodule.SaintDatamodule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SaintDatamodule</span></code></a>) – the datamodule that will preprocess the data before prediction</p></li>
<li><p><strong>df</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – the data taht will be used for prediction</p></li>
<li><p><strong>mc_dropout_iterations</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – number of iterations used for mcdropout estimation, if 0 it will not use</p></li>
</ul>
</dd>
</dl>
<p>mcdropout</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lit_saint.trainer.SaintTrainer.prefit">
<span class="sig-name descname"><span class="pre">prefit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lit_saint/trainer.html#SaintTrainer.prefit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lit_saint.trainer.SaintTrainer.prefit" title="Permalink to this definition"></a></dt>
<dd><p>Function that is used for the pretraining of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#lit_saint.model.Saint" title="lit_saint.model.Saint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Saint</span></code></a>) – instance of Saint that will be used for the pretraining</p></li>
<li><p><strong>datamodule</strong> (<a class="reference internal" href="#lit_saint.datamodule.SaintDatamodule" title="lit_saint.datamodule.SaintDatamodule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SaintDatamodule</span></code></a>) – instance of SaintDataModule that contains the data for the pretraining</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-lit_saint.version">
<span id="lit-saint-version-module"></span><h2>lit_saint.version module<a class="headerlink" href="#module-lit_saint.version" title="Permalink to this headline"></a></h2>
</section>
<section id="module-lit_saint">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-lit_saint" title="Permalink to this headline"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../example_regression.html" class="btn btn-neutral float-left" title="Example Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Luca Actis Grosso.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>